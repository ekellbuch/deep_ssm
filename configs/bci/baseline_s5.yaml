Comment: >
  # This trains the baseline model
  python run.py --config-name="baseline_s5" trainer_cfg.fast_dev_run=1 model_cfg.configs.n_layers=1 


project_name: "speech_bci"
experiment_name: "baseline_s5"

seed: 0

data_cfg:
  batchSize: 64
  num_workers: 0
  datasetPath: "${oc.env:DEEP_SSM_DATA}/SpeechBCI/competitionData/ptDecoder_ctc"
  train_split: 1    # % of training data assigned to train_loader 1=100%
  # transformations:
  whiteNoiseSD: 0.8
  constantOffsetSD: 0.2
  # masking
  mask_value: 0.0
  speckled_mask_p: 0.45
  feature_mask_p: 0.03
  temporal_mask_n: 1
  temporal_mask_len: 2
  renormalize_masking: false

module_cfg:
  type: "bci"

model_cfg:
  type: "bci_s5"
  resume_ckpt_path:   # load weights from ckpt but not parameters.
  configs:
    neural_dim: 256 #nInputFeatures
    n_classes: 40 #nClasses
    # s5 params
    ssm_size: 384
    bidirectional: true
    n_layers: 1 #nLayers
    dropout: 0.1 #dropout
    # base decoder parameters:
    gaussianSmoothWidth: 2.0
    unfolding: true
    strideLen: 4
    kernelLen: 32
    input_nonlinearity: "softsign"
    normalize_batch: false
    init_embedding_layer: false  # not implemented
    include_relu: false   # not implemented
    # other 
    fcc_layers: false  # not implemented
    #initialize_mixer: false  # not implemented
    

optimizer_cfg:
  type: "adam"
  configs:
    lr: 0.02 # lrStart
    betas: [0.9, 0.999]
    eps: 0.1
    weight_decay: 1e-5  #l2_decay

scheduler_cfg:
  type: "reduce_on_plateau"
  interval: "epoch"
  monitor: "ctc_loss_train"
  frequency: 1
  configs:
    patience: 5
    mode: "min"
    factor: 0.5
    min_lr: 1e-8

callbacks:
  lr_monitor:
    logging_interval: "step"
  grad_norm:
    type:
  early_stopping:
    monitor: "ctc_loss_train"
    min_delta: 1e-4
    patience: 30
    verbose: false
    mode: "min"
  masking_scheduler:
    masking_epochs: [ 25, 50]  # epochs where we reduce masking
    speckled_mask_p_schedule: [ 0.25, 0]
    feature_mask_p_schedule: [ 0.01, 0]
    temporal_mask_n_schedule: [ 0, 0]
  eigen_track:

trainer_cfg:
  fast_dev_run: false
  logger: "wandb"
  accelerator: "auto"
  max_steps: 15000
  log_every_n_steps: 100
  gradient_clip_val: 0
  gradient_clip_algorithm: "norm"
  limit_test_batches:
  limit_train_batches:
  limit_val_batches:
  

checkpoint_cfg:
  every_n_train_steps: 1000


eval_cfg:
  eval_only: 0 # set checkpoint path from which to resume job (reloads parameters)

hydra:
  run:
    dir: ./outputs/${now:%y-%m-%d}/${now:%H-%M-%S}
