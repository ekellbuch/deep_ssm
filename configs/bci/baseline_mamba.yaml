Comment: >
  # This trains the baseline model
  python run.py --config-name="baseline_mamba" trainer_cfg.fast_dev_run=1
  python run.py --config-name="baseline_mamba" trainer_cfg.limit_train_batches=1 trainer_cfg.limit_val_batches=1 trainer_cfg.limit_test_batches=1 trainer_cfg.log_every_n_steps=1

  # Experiment:

  python run.py --config-name="baseline_mamba" 
  python run.py --config-name="baseline_mamba" model_cfg.configs.input_nonlinearity=None

project_name: "speech_bci"
experiment_name: "baseline_mamba"


common:
  #outputDir: "/Users/ekellbuch/Projects/deepseq/bci/neural_seq_decoder/outputs"
  #datasetPath: &datasetPath "/Users/ekellbuch/datasets/SpeechBCI/competitionData/ptDecoder_ctc"
  outputDir: "/scratch/users/${oc.env:USER}/SpeechBCI/logs"
  datasetPath: &datasetPath "/scratch/groups/swl1/SpeechBCI/competitionData/ptDecoder_ctc"

module_cfg:
  type: "bci"

data_cfg:
  batchSize: 64
  num_workers: 0
  datasetPath: *datasetPath
  # train and val split
  train_split: 0.8
  # transformations:
  whiteNoiseSD: 0.8
  constantOffsetSD: 0.2
  # masking
  mask_value: 0.0
  speckled_mask_p: 0.45
  feature_mask_p: 0.03
  temporal_mask_n: 1
  temporal_mask_len: 2
  renormalize_masking: false


seed: 15

model_cfg:
  type: "bci_mamba"
  configs:
    neural_dim: 256 #nInputFeatures
    n_classes: 40 #nClasses
    # mamba params
    d_model: 1024
    d_state: 16
    d_conv: 4
    expand_factor: 1
    bidirectional_input: false
    bidirectional: true
    layer_dim: 6 #nLayers
    # base decoder parameters:
    gaussianSmoothWidth: 2.0
    unfolding: true
    strideLen: 4
    kernelLen: 32
    mamba_bi_new: true
    input_nonlinearity: "softsign"

optimizer_cfg:
  type: "adam"
  configs:
    lr: 0.01  # lrStart
    betas: [0.9, 0.99]
    eps: 0.1
    weight_decay: 1e-5  #l2_decay

scheduler_cfg:
  type: "reduce_on_plateau"
  monitor: "ctc_loss_validation"
  configs:
    mode: "min"

callbacks:
  lr_monitor:
    logging_interval: "step"
  grad_norm:
    type:
  early_stopping:
    monitor: "ctc_loss_validation"
    min_delta: 1e-4
    patience: 10
    verbose: false
    mode: "min"

trainer_cfg:
  fast_dev_run: false
  logger: "wandb"
  accelerator: "auto"
  max_steps: 10000
  log_every_n_steps: 100
  gradient_clip_val: 0 #.0
  gradient_clip_algorithm: "norm"
  limit_test_batches: 
  limit_train_batches:
  limit_val_batches:
  max_epochs:



eval_cfg:

hydra:
  run:
    dir: ./outputs/${now:%y-%m-%d}/${now:%H-%M-%S}
